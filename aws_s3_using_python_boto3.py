# -*- coding: utf-8 -*-
"""AWS S3 using python boto3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xO-JHYueWinrJtpjXtf9ZLGyWRjwJ8Re

# **Accessing AWS S3 using Python boto3 package**
"""

!pip install boto3

# Commented out IPython magic to ensure Python compatibility.
#setting environment variables

# %env AWS_ACCESS_KEY_ID=AKIAQPDEPZYI6JUA6X75
# %env AWS_SECRET_ACCESS_KEY=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
# %env AWS_DEFAULT_REGION=us-east-1

from google.colab import files
files.upload()

#creating an Amazon s3 bucket

import logging
import boto3
from botocore.exceptions import ClientError


def create_bucket(bucket_name, region=None):
    """Create an S3 bucket in a specified region

    If a region is not specified, the bucket is created in the S3 default
    region (us-east-1).

    :param bucket_name: Bucket to create
    :param region: String region to create bucket in, e.g., 'us-west-2'
    :return: True if bucket created, else False
    """

    # Creating bucket
    try:
        if region is None:
            s3_client = boto3.client('s3')
            s3_client.create_bucket(Bucket=bucket_name)
        else:
            s3_client = boto3.client('s3', region_name=region)
            location = {'LocationConstraint': region}
            s3_client.create_bucket(Bucket=bucket_name,
                                    CreateBucketConfiguration=location)
    except ClientError as e:
        logging.error(e)
        return False
    return True

create_bucket("doi077")

# Retrieve the list of existing buckets
import boto3
s3 = boto3.client('s3')
response = s3.list_buckets()

# Output the bucket names
print('Existing buckets:')
for bucket in response['Buckets']:
    print(f'  {bucket["Name"]}')

#To Upload the file in S3

def upload_file(file_name, bucket, object_name=None):
    """Upload a file to an S3 bucket

    :param file_name: File to upload
    :param bucket: Bucket to upload to
    :param object_name: S3 object name. If not specified then file_name is used
    :return: True if file was uploaded, else False
    """

    # If S3 object_name was not specified, use file_name
    if object_name is None:
        object_name = os.path.basename(file_name)

    # Upload the file
    s3_client = boto3.client('s3')
    try:
        response = s3_client.upload_file(file_name, bucket, object_name)
    except ClientError as e:
        logging.error(e)
        return False
    return True

# Uploading the readable file

s3 = boto3.client('s3')
with open("small_data.tsv", "rb") as f:
    s3.upload_fileobj(f, "doi077", "data.tsv")

def download_from(bucketname,objectname,filename):
  s3 = boto3.client('s3')
  s3.download_file(bucketname, objectname, filename)

download_from("doi077","data.tsv","guvidata_test.tsv")

import pandas as pd
df = pd.read_csv("small_data.tsv",sep = "\t")

# Creating a basic model to recommend movies from the smalldata file

from collections import Counter

def get_recommendations(user):
  watched_movies = df[df["user_id"]==user]["movie_id"]
  return ",".join([x for x in movie_suggestions if x not in watched_movies])

top_movies = Counter(df["movie_id"].values).most_common(10)
movie_suggestions = [x for (x,y) in top_movies]

get_recommendations(1)

# Creating a basic model to recommend movies from the smalldata file

import pandas as pd
import numpy as np

df = pd.read_csv("small_data.tsv",sep = "\t")

predictions = {}
users = df["user_id"].unique()
for user in users:
  try:
   predictions[str(user)] = get_recommendations(user)
  except:
    pass
print (type(predictions))

# To dump the code in AWS as json file and read it

pip install numpyencoder

from numpyencoder import NumpyEncoder

import os
import json

with open('result.json', 'w') as fp:
  json.dump(predictions, fp, cls=NumpyEncoder)

predictions.keys()

# To Upload the json file in Amazon s3

s3 = boto3.client('s3')
with open("result.json", "rb") as f:
  s3.upload_fileobj(f, "doi077", "data.json")